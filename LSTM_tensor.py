# -*- coding: utf-8 -*-
"""RNN-relu

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QhVo06JyZDooqWCP_miiMxGu0RRkf6KT
"""
# Import
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
import Data_load as dl
import os

current_path = os.getcwd()

DataX = list()
DataY = list()

sensor = 3

dl.Data_load("Sunny", "Animal", sensor, DataX, DataY)
dl.Data_load("Sunny", "Human", sensor, DataX, DataY)
dl.Data_load("Sunny", "Both", sensor, DataX, DataY)
dl.Data_load("Sunny", "None", sensor, DataX, DataY)
dl.Data_load("Rain", "Animal", sensor, DataX, DataY)
dl.Data_load("Rain", "Human", sensor, DataX, DataY)
dl.Data_load("Rain", "Both", sensor, DataX, DataY)
dl.Data_load("Rain", "None", sensor, DataX, DataY)

X_train, X_test, Y_train, Y_test = train_test_split(DataX, DataY, test_size=0.3)

print(np.array(DataX).shape)
print(np.array(DataY).shape)
print(np.array(X_test).shape)
print(np.array(Y_test).shape)

learning_rate = 0.0001
total_epoch = 200

n_input = 100
batch_size = 1
n_hidden = 128
n_class = 6

log_path = './Logs/RNN_CUSTOM/'
save_path = './Models/RNN'

tf.reset_default_graph()
# Set X, Y as place holder
X = tf.placeholder(tf.float32, [None, 1, n_input], name='X')
Y = tf.placeholder(tf.float32, [None, n_class], name='Y')
dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')

with tf.name_scope('weight'):
    W = tf.Variable(tf.random_uniform([n_hidden, n_class]), name='W')
    b = tf.Variable(tf.zeros([n_class]), name='b')

    W_hist = tf.summary.histogram('weight', W)
    b_hist = tf.summary.histogram('bias', b)

with tf.name_scope('RNN'):
    #  cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
    #  cell = tf.nn.rnn_cell.GRUCell(n_hidden)
    # To avoid overfitting
    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_rate)
    cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell2_LSTM')
    cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=dropout_rate)
    cell3 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell3_LSTM')
    cell3 = tf.nn.rnn_cell.DropoutWrapper(cell3, output_keep_prob=dropout_rate)

    # Make combination of cells
    multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell, cell2, cell3])
    multi_cell = tf.nn.rnn_cell.DropoutWrapper(multi_cell, output_keep_prob=dropout_rate)

    # Make  Deep RNN
    outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)
    outputs = tf.transpose(outputs, [1, 0, 2])
    outputs = outputs[-1]

    model = tf.add(tf.matmul(outputs, W), b, name="model")

#  model_hist = tf.summary.histogram('model', model)
with tf.name_scope('cost'):
    Y_pred = tf.contrib.layers.fully_connected(model, 6, activation_fn=tf.nn.relu)
    cost = tf.losses.mean_squared_error(Y, Y_pred)
    cost_summ = tf.summary.scalar('cost', cost)

with tf.name_scope('optimizer'):
    optimizer = tf.train.AdamOptimizer(learning_rate, name="optimizer").minimize(cost)

with tf.name_scope('accuracy'):
    # Print the Accuracy
    is_correct = tf.equal(tf.cast(tf.round(Y_pred), tf.float32), Y)
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
    tf.summary.scalar('accuracy', accuracy)

with tf.Session() as sess:
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter(log_path, sess.graph)

    sess = tf.Session()
    saver = tf.train.Saver()
    sess.run(tf.global_variables_initializer())

    # make input and target to train
    batch_xs = np.reshape(DataX, (-1, batch_size, n_input))
    batch_ys = DataY
    test_xs = np.reshape(X_test, (-1, batch_size, n_input))
    test_ys = Y_test

    X_data = np.array([
        [2215.0, 2123.0, 2051.0, 2019.0, 1941.0, 1810.0, 2091.0, 2146.0, 2203.0, 2202.0, 2204.0, 2129.0, 2065.0, 2032.0,
         1971.0, 1834.0, 1905.0, 2154.0, 2282.0, 2275.0, 2262.0, 2180.0, 2105.0, 2072.0, 1996.0, 1862.0, 1999.0, 2103.0,
         2258.0, 2317.0, 2308.0, 2236.0, 2153.0, 2093.0, 2002.0, 1850.0, 1909.0, 2144.0, 2256.0, 2237.0, 2192.0, 2138.0,
         2044.0, 2011.0, 1936.0, 1801.0, 1871.0, 2122.0, 2236.0, 2220.0, 2185.0, 2126.0, 2051.0, 2008.0, 1933.0, 1799.0,
         2088.0, 2091.0, 2092.0, 2194.0, 2208.0, 2187.0, 2096.0, 2048.0, 1961.0, 1816.0, 1862.0, 2092.0, 2207.0, 2176.0,
         2126.0, 2085.0, 1995.0, 1944.0, 1859.0, 1722.0, 1798.0, 2028.0, 2144.0, 2133.0, 2092.0, 2048.0, 1964.0, 1920.0,
         1838.0, 1703.0, 1841.0, 1941.0, 2114.0, 2169.0, 2157.0, 2107.0, 2016.0, 1946.0, 1848.0, 1710.0, 1779.0, 2013.0,
         2132.0, 2126.0], [
            1834.0, 1905.0, 2154.0, 2282.0, 2275.0, 2262.0, 2180.0, 2105.0, 2072.0, 1996.0, 1862.0, 1999.0, 2103.0,
            2258.0,
            2317.0, 2308.0, 2236.0, 2153.0, 2093.0, 2002.0, 1850.0, 1909.0, 2144.0, 2256.0, 2237.0, 2192.0, 2138.0,
            2044.0,
            2011.0, 1936.0, 1801.0, 1871.0, 2122.0, 2236.0, 2220.0, 2185.0, 2126.0, 2051.0, 2008.0, 1933.0, 1799.0,
            2088.0,
            2091.0, 2092.0, 2194.0, 2208.0, 2187.0, 2096.0, 2048.0, 1961.0, 1816.0, 1862.0, 2092.0, 2207.0, 2176.0,
            2126.0,
            2085.0, 1995.0, 1944.0, 1859.0, 1722.0, 1798.0, 2028.0, 2144.0, 2133.0, 2092.0, 2048.0, 1964.0, 1920.0,
            1838.0,
            1703.0, 1841.0, 1941.0, 2114.0, 2169.0, 2157.0, 2107.0, 2016.0, 1946.0, 1848.0, 1710.0, 1779.0, 2013.0,
            2132.0,
            2126.0, 2080.0, 2046.0, 1964.0, 1921.0, 1840.0, 1709.0, 2008.0, 2053.0, 2116.0, 2106.0, 2081.0, 2048.0,
            1989.0,
            1957.0, 1876.0], [
            2308.0, 2236.0, 2153.0, 2093.0, 2002.0, 1850.0, 1909.0, 2144.0, 2256.0, 2237.0, 2192.0, 2138.0, 2044.0,
            2011.0,
            1936.0, 1801.0, 1871.0, 2122.0, 2236.0, 2220.0, 2185.0, 2126.0, 2051.0, 2008.0, 1933.0, 1799.0, 2088.0,
            2091.0,
            2092.0, 2194.0, 2208.0, 2187.0, 2096.0, 2048.0, 1961.0, 1816.0, 1862.0, 2092.0, 2207.0, 2176.0, 2126.0,
            2085.0,
            1995.0, 1944.0, 1859.0, 1722.0, 1798.0, 2028.0, 2144.0, 2133.0, 2092.0, 2048.0, 1964.0, 1920.0, 1838.0,
            1703.0,
            1841.0, 1941.0, 2114.0, 2169.0, 2157.0, 2107.0, 2016.0, 1946.0, 1848.0, 1710.0, 1779.0, 2013.0, 2132.0,
            2126.0,
            2080.0, 2046.0, 1964.0, 1921.0, 1840.0, 1709.0, 2008.0, 2053.0, 2116.0, 2106.0, 2081.0, 2048.0, 1989.0,
            1957.0,
            1876.0, 1751.0, 1835.0, 2070.0, 2184.0, 2170.0, 2130.0, 2085.0, 2018.0, 1969.0, 1888.0, 1760.0, 1844.0,
            2088.0,
            2205.0, 2185.0]
    ])

    for epoch in range(total_epoch):
        summary, _, loss = sess.run([merged, optimizer, cost],
                                    feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.8})
        writer.add_summary(summary, epoch)

        if epoch % 1 == 0:
            print('Epoch:', '%04d' % (epoch + 1),
                  'Avg. cost =', '{:.3f}'.format(loss))

    print('최적화 완료!')

saver.save(sess, save_path, total_epoch)

print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 1}))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: test_xs, Y: test_ys, dropout_rate: 1}))

